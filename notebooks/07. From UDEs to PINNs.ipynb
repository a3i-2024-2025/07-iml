{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Notebook setup: run this before everything\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Control figure size\n",
    "figsize=(14, 4)\n",
    "\n",
    "from util import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From UDEs to PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## UDEs and Similar Approaches\n",
    "\n",
    "**UDEs provide a good starting point for _two more approaches_**\n",
    "\n",
    "If you keep the connection to physics, but your _relax the ODE mechanism_\n",
    "\n",
    "* ...Then you get _Physics Informed Neural Networks_\n",
    "  - Technically, UDEs can be considered PINNs\n",
    "  - ...But the term refers typically to the approaches surveyed (e.g.) [here](https://link.springer.com/article/10.1007/s10915-022-01939-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you keep the ODE mechanism, but you _drop the connection to physics_\n",
    "\n",
    "* ...Then you get _Neural Ordinary Differential Equations_\n",
    "  - This was the first approach to integrate NNs and differential equations\n",
    "  - The seminal paper is [publicly available](https://proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html)\n",
    "  \n",
    "**We are going to briefly outline the former approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From UDEs to PINNs\n",
    "\n",
    "**Let's start by recapping how UDEs work**\n",
    "\n",
    "At _inference time_, we (typically) integrate an initial value problem:\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\dot{\\hat{y}} = f(\\hat{y}, t, U(\\hat{y}, t, \\theta)) \\\\\n",
    "& \\hat{y}(0) = y_0\n",
    "\\end{align}$$\n",
    "\n",
    "At _training time_, we solve:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin}_\\theta\\ & L(\\hat{y}(t), y) \\\\\n",
    "& \\dot{\\hat{y}} = f(\\hat{y}, t, U(\\hat{y}, t, \\theta)) \\\\\n",
    "& \\hat{y}(t_0) = y_0\n",
    "\\end{align}$$\n",
    "\n",
    "* Which requires to embed ODE integration in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From UDEs to PINNs\n",
    "\n",
    "**What if we tried to simplify the inference process?**\n",
    "\n",
    "For example, we could use a NN to _approximate $y(t)$ itself_\n",
    "\n",
    "$$\n",
    "\\hat{y}(t; \\theta) \\simeq y(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This approach has several immediate benefits:**\n",
    "\n",
    "* Inference becomes as efficient as evaluating $\\hat{y}(t; \\theta)$\n",
    "  - No need to integrate anything, linear scalability w.r.t. the sampling points\n",
    "* Handling PDEs also becomes pretty simple\n",
    "  - We just need to use a multivariate $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But where is physics here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training in PINNs\n",
    "\n",
    "**The ODE is taken into account _at training time_**\n",
    "\n",
    "Superficially, the training problem is similar to the UDE one:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin}_\\theta\\ & L(\\hat{y}(t, \\theta), y) \\\\\n",
    "& \\dot{\\hat{y}}(t; \\theta) = f(\\hat{y}(t; \\theta), t) \\\\\n",
    "& \\hat{y}(t_0; \\theta) = y_0\n",
    "\\end{align}$$\n",
    "\n",
    "...But in fact, the situation is very different:\n",
    "\n",
    "* Since both $\\hat{y}(t; \\theta)$ and $\\dot{\\hat{y}}(t; \\theta)$ need to be learned\n",
    "* ...Classical ODE integration methods are no longer viable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PINNs circumvent this issue by _using NN training for ODE integration_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training in PINNs\n",
    "\n",
    "**In particular, we can apply a Lagrangian relaxation to the problem**\n",
    "\n",
    "We relax the constraints in the previous formulation so that we obtain:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{L}(y, \\hat{y}, t, \\theta) & = L(\\hat{y}(t; \\theta), y) \\\\\n",
    "                          & + \\lambda_{de}^T \\|\\dot{\\hat{y}}(t; \\theta) - f(\\hat{y}(t; \\theta), t)\\|_2^2 \\\\\n",
    "                          & + \\lambda_{bc}^T \\|\\hat{y}(t_0; \\theta) - y_0\\|_2^2\n",
    "\\end{align}$$\n",
    "\n",
    "In optimization, this is called a _Lagrangian_\n",
    "\n",
    "* Besides the original loss $L$\n",
    "* ...There is a penalty term linked to the ODE, with weights (multipliers) $\\lambda_{de}^T$\n",
    "* ...And a penalty term linked to the initial value, with multipliers $\\lambda_{bc}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training in PINNs\n",
    "\n",
    "**The approach can be generalized**\n",
    "\n",
    "* In particular we can take into account both ODEs and PDEs\n",
    "* ...And we can use different types of penalizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We just need to abstract a bit the formulation:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{L}(y, \\hat{y}, t, \\theta) & = L(\\hat{y}(t; \\theta), y) \\\\\n",
    "                          & + \\lambda_{de}^T L_{de}(F(\\hat{y}, t; \\theta)) \\\\\n",
    "                          & + \\lambda_{bc}^T L_{bc}(B(\\hat{y}, t; \\theta))\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "* Where $F(y, t; \\theta) = 0$ defines the original ordinary or partial DE\n",
    "* ...And $B(y, t; \\theta) = 0$ defines the original initial or boundary conditions\n",
    "* The $L_{de}$ and $L_{bc}$ terms can be L2 norms, but also other types of penalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training in PINNs\n",
    "\n",
    "**Then we train by:**\n",
    "\n",
    "* Sampling points $\\{t_i\\}_{i=1}^n$ in the input space\n",
    "* Choosing $\\theta$ so as to minimize the sum of Lagrangians\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{argmin}_\\theta \\sum_{i=1}^n \\mathcal{L}(y, \\hat{y}, t, \\theta)\n",
    "\\end{align}$$\n",
    "\n",
    "We can employ gradient descent, as usual\n",
    "\n",
    "**Again, there is no need to use ODE/PDE integration at training time**\n",
    "\n",
    "...Because _training is the integration process_\n",
    "\n",
    "* In fact, it is possible to drop the data-based loss $L$ and the approach still works\n",
    "* In such a case, PINNs can act as _approximate_ ODE/PDE integrators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## No Free Lunch\n",
    "\n",
    "**In the above description, it's easy to miss an important point**\n",
    "\n",
    "Let's consider again the DE-based components in the Lagrangian:\n",
    "\n",
    "$$\n",
    "L_{de}(F(\\hat{y}, t, \\theta)) \\quad \\text{ which could be e.g. } \\quad \\|\\color{red}{\\dot{\\hat{y}}(t; \\theta)} - f(\\hat{y}(t; \\theta), t)\\|_2^2\n",
    "$$\n",
    "\n",
    "* The penalizer contains _derivatives_ (possibly partial)\n",
    "* ...And it should provide a contribution for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This means that we need a way to compute the components of $\\dot{y}$**\n",
    "\n",
    "...So that we obtain an expression that is _again differentiable in $\\theta$_\n",
    "\n",
    "* This can be a bit tricky in practice!\n",
    "* Viable approaches include symbolic differentiation (manual or automatic)\n",
    "* ...Or partially numeric methods such as finite differences, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## No Free Lunch\n",
    "\n",
    "**Moreover, assigning a value to the multipliers is not trivial**\n",
    "\n",
    "These the \"weight\" vectors $\\lambda_{de}$ and $\\lambda_{bc}$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{L}(y, \\hat{y}, t, \\theta) = L(\\hat{y}(t; \\theta), y)\n",
    "                            + \\color{red}{\\lambda_{de}^T} L_{de}(F(\\hat{y}, t;\\theta))\n",
    "                          + \\color{red}{\\lambda_{bc}^T} L_{bc}(B(\\hat{y}, t; \\theta))\n",
    "\\end{align}$$\n",
    "\n",
    "Finding a good balance might be very tricky\n",
    "\n",
    "* A good alternative might be using dual ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, boundary conditions are _incorporated at training time_**\n",
    "\n",
    "...So, if they change, we need to _repeat training_\n",
    "\n",
    "* In some contexts, this can be a major problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## No Free Lunch\n",
    "\n",
    "**Finally, one should be careful with the problem semantic**\n",
    "\n",
    "Let's consider for a given input vector $t$ the constraint:\n",
    "\n",
    "$$\n",
    "\\|\\dot{\\hat{y}}(t; \\theta) - f(\\hat{y}(t; \\theta), t)\\|_2^2\n",
    "$$\n",
    "\n",
    "**The constraint is enforced in a _soft_ fashion**\n",
    "\n",
    "...Meaning that it might be violated\n",
    "\n",
    "* Proper weight calibration can help, but violations will still typically occur\n",
    "\n",
    "**Even if we manage exact satisfaction**\n",
    "\n",
    "...The constraint will hold only locally, for the specified $t$ values\n",
    "\n",
    "* When we move away from the $t$ values considered at training time\n",
    "* ...The NN may behave inconsistently with the underlying physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Remarks\n",
    "\n",
    "**Let's conclude with some differences betweenn mainstream PINNs and UDEs**\n",
    "\n",
    "Unlike UDEs, PINNs need to _learn the involved physics_\n",
    "\n",
    "* It might be necessary to use larger networks\n",
    "* ...Because they will need to learn a more complex relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DE constraints are only _approximately satisfied_\n",
    "\n",
    "* UDEs provide instead full guarantees\n",
    "* ...But approximate satisfisfaction might be good if the DE is not fully reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "PINNs do not rely on DE integration: they _are_ an integration method\n",
    "\n",
    "* This makes them faster than UDEs at inference and possibly training time\n",
    "* ...But don't forget that changing the boundary conditions requires retraining!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Remarks\n",
    "\n",
    "**If you are looking for additional information**\n",
    "\n",
    "* There's a very well done [PyTorch library for PINNs](https://github.com/mathLab/PINA)\n",
    "* A well-known library is also [available for JAX](https://docs.kidger.site/diffrax/)\n",
    "* The PINN idea can be generalized, leading to [Neural Operators](https://arxiv.org/pdf/2010.08895)\n",
    "* ...Which map boundary conditions into integrated differential equations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "center": false,
   "enable_chalkboard": true,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
