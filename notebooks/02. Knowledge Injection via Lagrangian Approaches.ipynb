{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook setup\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Control figure size\n",
    "figsize=(14, 4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Constrained ML via Lagrangian Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constraints as Penalties\n",
    "\n",
    "**Let's consider our general situation**\n",
    "\n",
    "We have a training problem:\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_{\\theta} \\left\\{ L(\\hat{y}) \\mid \\hat{y} = f(x; \\theta) \\right\\}\n",
    "$$\n",
    "\n",
    "And constraints, that we'll view as an inequality on a vector function\n",
    "\n",
    "$$\n",
    "g(\\hat{y}) \\leq 0\n",
    "$$\n",
    "\n",
    "* Here $g(\\hat{y}) = \\{g_k(\\hat{y})\\}_{k=1}^{m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We will explore the idea of turning the constraints into _loss terms_**\n",
    "\n",
    "* Doing tha will steer the model towards satisfying the constraints\n",
    "* ...And can be thought of as a form of regularization\n",
    "\n",
    "In fact, an early example of this approach is called [Semantic Based Regularization](https://www.sciencedirect.com/science/article/pii/S0004370215001344)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lagrangian-like Loss\n",
    "\n",
    "**The basic theory is rooted in _Lagrangian duality_**\n",
    "\n",
    "...Where our constrained optimization problem would be turned into:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = L(\\hat{y}) + \\lambda^T g(\\hat{y})\n",
    "$$\n",
    "\n",
    "* Where $\\lambda$ is a vector of weigths $\\in \\mathbb{R}^m$, called Lagrangian multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **However, this formulation is _not a good choice_ in our case**\n",
    "\n",
    "* There are a few reasons for this, non of them trivial\n",
    "* Here we will focus just on the main one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Stop-gain Mechanism\n",
    "\n",
    "**We are considering inequality constraints**\n",
    "\n",
    "$$\n",
    "g(\\hat{y}) \\leq 0\n",
    "$$\n",
    "\n",
    "* Predictions with $g_k(\\hat{y}) < 0$ are equivalent to those with $g_k(\\hat{y}) = 0$\n",
    "* ...But in a classical Lagrangian approach a slack translates to a _reward_\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = L(\\hat{y}) + \\lambda^T \\underbrace{g(\\hat{y})}_{< 0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In classical Lagrangian theory, this is countered by _changing the sign of $\\lambda_k$_**\n",
    "\n",
    "* However, this is sound only under specific assumptions (e.g. convexity)\n",
    "* ...And it requires to optimize $\\theta$ and $\\lambda$ simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clipped Penalizers\n",
    "\n",
    "**However, there's a far easier alternative**\n",
    "\n",
    "We can just use non-linearity to remove the reward effect, e.g. by clipping:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = L(\\hat{y}) + \\lambda^T \\max(0, g(\\hat{y}))\n",
    "$$\n",
    "\n",
    "* The maximum operator will neutralized any reward when $g_k(\\hat{y}) < 0$\n",
    "* ...Which is effectively equivalent to forcing $\\lambda_k$ to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**With the new penalizer, _for any $\\lambda \\geq 0$_**\n",
    "\n",
    "* When all constraints are feasible, we preserve the original loss function\n",
    "* When a constraint is infeasible, we introduce a penalty\n",
    "\n",
    "\n",
    "> **This approach comes from [penalty methods](https://en.wikipedia.org/wiki/Penalty_method#:~:text=Penalty%20methods%20are%20a%20certain,of%20the%20original%20constrained%20problem.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Equality Constraints\n",
    "\n",
    "**Equality constraints allow for a simpler formulation**\n",
    "\n",
    "In principle, given an equality constraint:\n",
    "\n",
    "$$\n",
    "g_k(\\hat{y}) = 0\n",
    "$$\n",
    "\n",
    "We can state it as two inequality constraints:\n",
    "\n",
    "$$\n",
    "g_k(\\hat{y}) \\leq 0 \\quad \\text{and} \\quad -g_k(\\hat{y}) \\leq 0\n",
    "$$\n",
    "\n",
    "...And build two (weighted) violation terms:\n",
    "\n",
    "$$\n",
    "\\lambda_k' \\max\\left(0, g_k(\\hat{y})\\right) \\quad \\text{and} \\quad \\lambda_k''  \\max\\left(0, -g_k(\\hat{y})\\right)\n",
    "$$\n",
    "\n",
    "* With $\\lambda_k', \\lambda_k'' \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Equality Constraints\n",
    "\n",
    "**Summing the two terms leads to a simplified formula**\n",
    "\n",
    "$$\n",
    "\\lambda_k' \\max\\left(0, g_k(\\hat{y})\\right) + \\lambda_k''  \\max\\left(0, -g_k(\\hat{y})\\right) = \\lambda_k |g_k(\\hat{y})|\n",
    "$$\n",
    "\n",
    "* Where $\\lambda_k = \\lambda_k' + \\lambda_k''$ and there is no sign restriction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Another common form relies on the _square_ of the violation**\n",
    "\n",
    "...Meaning that we consider the loss:\n",
    "\n",
    "$$\n",
    "L(\\hat{y}) + \\lambda^T g(\\hat{y})^2\n",
    "$$\n",
    "\n",
    "This form of penalizer is related to properties of the Normal distribution\n",
    "\n",
    "* It is particularly well-suited for soft constraints\n",
    "* ...But we won't discuss the connection in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Differentiability\n",
    "\n",
    "**It's worth talking about differentiability**\n",
    "\n",
    "* Lagrangian approaches for knowledge injection\n",
    "* ...Are most common with differentiable constraints\n",
    "\n",
    "...Even if differentiability is _not strictly needed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Differentiability _might_ be needed**\n",
    "\n",
    "...Depending on which training algorithms is used, e.g.:\n",
    "\n",
    "* Gradient descent\n",
    "* Gradient boosting\n",
    "* ...\n",
    "\n",
    "...Which means that we need differentiability when using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Calibrating the Multipliers\n",
    "\n",
    "**Any $\\lambda \\geq 0$ results in a sound behavior, but which value should we pick?**\n",
    "\n",
    "* Larger values will likely allow for some degree violation\n",
    "* Under proper assumptions, larger values can guarantee satisfaction\n",
    "\n",
    "The best strategy depends on our goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**If the system value stems from its _accuracy_**\n",
    "\n",
    "* Then the constraints are just a source of symbolic knowledge\n",
    "* ...And we will typically work with _soft constraints_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If _satisfying constraints_ is a value per-se**\n",
    "\n",
    "* Then the constraints are our main goal\n",
    "* ...And we will typically want _hard constraints_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Calibrating $\\lambda$ for Maximal Accuracy\n",
    "\n",
    "**When the goal is improving accuracy**\n",
    "\n",
    "...We can just assess the quality of a $\\lambda$ vector by _cross-validation_\n",
    "\n",
    "* Then $\\lambda$ can be thought of as any other ML hyperparameter\n",
    "* ...And we can optmize it via grid search, Baysian optimization, etc.\n",
    "\n",
    "In practice, however, this approach _doest not always work_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**In most cases, knowledge injection is used when _supervised data is scarce_**\n",
    "\n",
    "...And in this situation cross-validation is not very reliable\n",
    "\n",
    "* Then we $\\lambda$ can be calibrated via probabilistic considerations\n",
    "* ...Or via heuristic considerations\n",
    "\n",
    "Both approaches are not ideal, but at least they are applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Calibrating $\\lambda$ for Constraint Satisfaction\n",
    "\n",
    "**If our main goal is constraint satisfation**\n",
    "\n",
    "...Then we might think of just choosing a very large $\\lambda$\n",
    "\n",
    "* Intuitively, for sufficiently large $\\lambda$ values\n",
    "* ...We should at least approach constraint satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In practice, this almost never a good idea**\n",
    "\n",
    "Overly large $\\lambda$ values may lead to numerical issues:\n",
    "\n",
    "$$\n",
    "L(y, f(x, \\theta)) +\\lambda^T \\max\\left(0, g(f(x, \\theta)) \\right)\n",
    "$$\n",
    "\n",
    "* Any time we have a constraint violation\n",
    "* ...The gradient may include a disproportionaly large term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Ascent to Control the Multipliers\n",
    "\n",
    "**A gentler approach consists in using _gradient ascent for the multipliers_**\n",
    "\n",
    "Let's consider our modified loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\lambda) = L(y, f(x, \\theta)) +\\lambda^T \\max\\left(0, g(f(x, \\theta)) \\right)\n",
    "$$\n",
    "\n",
    "* This is actually differentiable in $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The gradient is also a very simple expression:**\n",
    "\n",
    "$$\n",
    "\\nabla_{\\lambda} \\mathcal{L}(\\theta, \\lambda) = \\max\\left(0, g(f(x, \\theta))\\right)\n",
    "$$\n",
    "\n",
    "* For satisfied constraints, the partial derivative is 0\n",
    "* For violated constraints, it is equal to the violation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Dual Ascent Method\n",
    "\n",
    "**Therefore, we can solve our constrained ML problem**\n",
    "\n",
    "...By alternating _gradient descent and ascent_:\n",
    "\n",
    "* $\\lambda^{(0)} = 0$\n",
    "* For $k = 1..n$ (or until convergence):\n",
    "  - Obtain $\\lambda^{(k)}$ via an ascent step with $\\nabla_{\\lambda} \\mathcal{L}(\\lambda, \\theta^{(k-1)})$\n",
    "  - Obtain $\\theta^{(k)}$ via a descent step with $\\nabla_{\\theta} \\mathcal{L}(\\lambda^{(k)}, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**We might still reach impractical values for $\\lambda$**\n",
    "\n",
    "...But the gentle updates can keep the gradient more stable\n",
    "\n",
    "* At the beginning, SGD will be free to prioritize accuracy\n",
    "* After some iterations, both $\\theta$ and $\\lambda$ will be nearly (locally) optimal"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
